{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from nltk import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "#import string\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "import numpy as np \n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splite data train and valid\n",
    "def data_splite (datapath):\n",
    "    headers = ['polarity', 'headline', 'content']\n",
    "    df = pd.read_csv(datapath+'/row_data/train.csv', encoding=\"utf-8\",sep=\",\", names=headers)\n",
    "    train, valid = train_test_split(df, test_size=0.2)\n",
    "    pd.DataFrame(train).to_csv(datapath+ '/train.csv', sep=',', encoding='utf-8')\n",
    "    pd.DataFrame(valid).to_csv(datapath+ '/valid.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "data_splite('C:/Users/aso/Documents/WPy-376/notebooks/NLP - Python/data')\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    \"\"\" Download the date: list of texts with scores.\"\"\"\n",
    "    headers = ['polarity', 'headline', 'content']\n",
    "    sentences = pd.read_csv(filename, encoding=\"utf-8\",sep=\",\", names=headers)\n",
    "    sentences['text'] = sentences['headline'] + sentences['content']\n",
    "    sentences.drop(columns=[col for col in sentences.columns if col not in ['polarity','text']], inplace=True)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labelset = None\n",
    "        self.label_binarizer = LabelBinarizer()\n",
    "        self.model = None\n",
    "        self.epochs = 22\n",
    "        self.batchsize = 64\n",
    "        self.max_features = 8000\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=self.max_features,\n",
    "            analyzer=\"word\",\n",
    "            tokenizer=self.mytokenize,\n",
    "            stop_words=None,\n",
    "            ngram_range=(1,2),\n",
    "            binary=False,\n",
    "            preprocessor=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytokenize(self, text):\n",
    "    \n",
    "    # split into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "        \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation[0:6])\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation[7:])\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "        \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "        \n",
    "    # filter out specifique tokens\n",
    "    alpha=set(['the','s'])\n",
    "    tokens = [w for w in tokens if not w in alpha]\n",
    "       \n",
    "    # stemming of words\n",
    "    porter = FrenchStemmer()\n",
    "    tokens = [porter.stem(w) for w in tokens]\n",
    "        \n",
    "    # define vocab\n",
    "    vocab = Counter() \n",
    "    vocab.update(tokens) \n",
    "    # keep tokens with a max occurrence    \n",
    "    max_occurane = 14\n",
    "    tokens = [k for k,c in vocab.items() if c <= max_occurane] \n",
    "\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "    def feature_count(self):\n",
    "        return len(self.vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "    def create_model(self):\n",
    "        input = Input((self.feature_count(),))\n",
    "        layer = input\n",
    "        layer = Dense(32, activation='tanh')(layer)\n",
    "        output = Dense(len(self.labelset), activation='softmax')(layer)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        model.compile(optimizer=optimizers.Adam(),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    def vectorize(self, texts):\n",
    "        return self.vectorizer.transform(texts).toarray()\n",
    "\n",
    "\n",
    "    def train_on_data(self, texts, labels, valtexts=None, vallabels=None):\n",
    "        Y_train = self.label_binarizer.fit_transform(labels)\n",
    "        self.labelset = set(self.label_binarizer.classes_)\n",
    "        print(\"LABELS: %s\" % self.labelset)\n",
    "        self.vectorizer.fit(texts)\n",
    "        # create a model to train\n",
    "        self.model = self.create_model()\n",
    "        # for each text example, build its vector representation\n",
    "        X_train = self.vectorize(texts)\n",
    "        #\n",
    "        my_callbacks = []\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None)\n",
    "        my_callbacks.append(early_stopping)\n",
    "        if valtexts is not None and vallabels is not None:\n",
    "            X_val = self.vectorize(valtexts)\n",
    "            Y_val = self.label_binarizer.transform(vallabels)\n",
    "            valdata = (X_val, Y_val)\n",
    "        else:\n",
    "            valdata = None\n",
    "        # Train the model!\n",
    "        self.model.fit(\n",
    "            X_train, Y_train,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batchsize,\n",
    "            callbacks=my_callbacks,\n",
    "            validation_data=valdata,\n",
    "            verbose=2)\n",
    "\n",
    "    def predict_on_X(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_on_data(self, texts):\n",
    "        X = self.vectorize(texts)\n",
    "        Y = self.model.predict(X)\n",
    "        return self.label_binarizer.inverse_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class train & predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, trainfile, valfile=None):\n",
    "        df = load_dataset(trainfile)\n",
    "        texts = df['text']\n",
    "        labels = df['polarity']\n",
    "        if valfile:\n",
    "            valdf = load_dataset(valfile)\n",
    "            valtexts = valdf['text']\n",
    "            vallabels = valdf['polarity']\n",
    "        else:\n",
    "            valtexts = vallabels = None\n",
    "        self.train_on_data(texts, labels, valtexts, vallabels)\n",
    "\n",
    "\n",
    "    def predict(self, datafile):\n",
    "        items = load_dataset(datafile)\n",
    "        return self.predict_on_data(items['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_reproducible():\n",
    "    # The below is necessary to have reproducible behavior.\n",
    "    import random as rn\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(17)\n",
    "    rn.seed(12345)\n",
    "\n",
    "def eval_list(glabels, slabels):\n",
    "    if (len(glabels) != len(slabels)):\n",
    "        print(\"\\nWARNING: label count in system output (%d) is different from gold label count (%d)\\n\" % (\n",
    "        len(slabels), len(glabels)))\n",
    "    n = min(len(slabels), len(glabels))\n",
    "    incorrect_count = 0\n",
    "    for i in range(0, n):\n",
    "        if slabels[i] != glabels[i]: incorrect_count += 1\n",
    "    acc = (n - incorrect_count) / n\n",
    "    acc = acc * 100\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_and_eval_dev_test(trainfile, devfile, testfile, run_id):\n",
    "    #classifier = Classifier()\n",
    "    print(\"\\n\")\n",
    "    # Training\n",
    "    print(\"RUN: %s\" % str(run_id))\n",
    "    print(\"  %s.1. Training the classifier...\" % str(run_id))\n",
    "    train(trainfile, devfile)\n",
    "    print()\n",
    "    print(\"  %s.2. Evaluation on the dev dataset...\" % str(run_id))\n",
    "    slabels = predict(devfile)\n",
    "    glabels = load_dataset(devfile)\n",
    "    glabels = glabels['polarity']\n",
    "    devacc = eval_list(glabels, slabels)\n",
    "    print(\"       Acc.: %.2f\" % devacc)\n",
    "    testacc = -1\n",
    "    if testfile is not None:\n",
    "        # Evaluation on the test data\n",
    "        print(\"  %s.3. Evaluation on the test dataset...\" % str(run_id))\n",
    "        slabels = predict(testfile)\n",
    "        glabels = load_dataset(devfile)\n",
    "        glabels = glabels['polarity']\n",
    "        testacc = eval_list(glabels, slabels)\n",
    "        print(\"       Acc.: %.2f\" % testacc)\n",
    "    print()\n",
    "    return (devacc, testacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_reproducible()\n",
    "#datadir = \"../data/\"\n",
    "datadir = \"C:/Users/aso/Documents/WPy-376/notebooks/NLP - Python/data/\"\n",
    "trainfile =  datadir + \"train.csv\"\n",
    "devfile =  datadir + \"valid.csv\"\n",
    "testfile =  datadir + \"test.csv\"\n",
    "#testfile = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RUN: 1\n",
      "  1.1. Training the classifier...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'train_on_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-63402309c8c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtestaccs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_eval_dev_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mdevaccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtestaccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-662f877a6066>\u001b[0m in \u001b[0;36mtrain_and_eval_dev_test\u001b[1;34m(trainfile, devfile, testfile, run_id)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RUN: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  %s.1. Training the classifier...\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  %s.2. Evaluation on the dev dataset...\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-010b4b926bb1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, trainfile, valfile)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mvaltexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvallabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvaltexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvallabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'train_on_data'"
     ]
    }
   ],
   "source": [
    "# Basic checking\n",
    "start_time = time.perf_counter()\n",
    "n = 2\n",
    "devaccs = []\n",
    "testaccs = []\n",
    "for i in range(n):\n",
    "    res = train_and_eval_dev_test(trainfile, devfile, testfile, i+1)\n",
    "    devaccs.append(res[0])\n",
    "    testaccs.append(res[1])\n",
    "    print('\\nCompleted %d runs.' % n)\n",
    "    print(\"Dev accs:\", devaccs)\n",
    "    print(\"Test accs:\", testaccs)\n",
    "    print()\n",
    "    print(\"Mean Dev Acc.: %.2f (%.2f)\\tMean Test Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs), np.mean(testaccs), np.std(testaccs)))\n",
    "    print(\"\\nExec time: %.2f s.\" % (time.perf_counter()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
